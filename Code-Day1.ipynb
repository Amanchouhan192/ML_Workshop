{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/asd/Desktop/Winter Training/data1.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we convert the data into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npData=np.asarray(data)\n",
    "#print(npData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the dimensions, size and shape of the array of data. npData represents the whole data array of 20000X20 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndim data 2\n",
      "size data 400000\n",
      "shape data (20000, 20)\n"
     ]
    }
   ],
   "source": [
    "print('ndim data', npData.ndim)\n",
    "print('size data', npData.size)\n",
    "print('shape data', npData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shape here says that there are 20000 datapoints or inputs we can give to the neural network\n",
    "\n",
    "20 here suggests that there are 20 different feutures of the data which we are studying and hence signifies the number of input neurons\n",
    "\n",
    "Doing the same for labels.csv and saving the array of corresponding labels of the data 20000X1 into a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label=pd.read_csv('C:/Users/asd/Desktop/Winter Training/label1.csv',header=None)\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape label (20000, 1)\n",
      "size label 20000\n",
      "ndim label 2\n"
     ]
    }
   ],
   "source": [
    "npLabel=np.asarray(label)\n",
    "print('shape label', npLabel.shape)\n",
    "print('size label', npLabel.size)\n",
    "print('ndim label', npLabel.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [2]\n",
      " [2]\n",
      " ..., \n",
      " [2]\n",
      " [2]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(npLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the labels to oneHot vector format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert our labels in the one hot format where we convert each labelinto a matrix so that all numbers in the label can be clasified individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n_values = np.max(npLabel) + 1\n",
    "oneHotDash=np.eye(n_values)[npLabel]\n",
    "oneHot=np.reshape(oneHotDash,(20000,5))\n",
    "print(oneHot.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into Training data, validation data and testing data inpropotion 80:10:10 using train_test_split function.\n",
    "\n",
    "We can use data slicing as well - `trainData`= `npData[0:16000]`, `validData`=`npData[16000:18000]`, `testData`=`npData[18000:20000]`\n",
    "\n",
    "and for labels, `trainLabel`= `npLabel[0:16000]`, `validLabel`=`npLabel[16000:18000]`, `testLabel`=`npLabel[18000:20000]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainData,testDataDash=train_test_split(npData,train_size=0.8,test_size=0.2,shuffle=False)#divide data in a 80:20 ratio\n",
    "trainLabel,testLabelDash=train_test_split(oneHot,train_size=0.8,test_size=0.2,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validateData, testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)#dividing rest20% data half half in validation and testing data\n",
    "validateLabel, testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 neurons in the input layer.\n",
    "\n",
    "There are 5 neurons in the output layer\n",
    "\n",
    "There are 10 neurons in the hidden layer\n",
    "\n",
    "Here we take input of the number of neurons in the hidden layer of our single layered neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "h=input()#number of neurons in the hidden layer\n",
    "h=int(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases are initialized to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We make randomized weight matricesaccording to random normal distribution which is according to 0 mean and 1/nL variance or1/sqrt(nL) standard deviation where nL if the number of inuts to that particular laye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std1,std2 0.22360679775 0.316227766017\n",
      "[[ 0.20617593 -0.04174916 -0.19067534 -0.18649261  0.07703732 -0.0990274\n",
      "   0.27895485  0.02208525  0.03450309  0.40240765]\n",
      " [-0.19054476  0.23976541 -0.09972385  0.099872   -0.3299326   0.16675107\n",
      "  -0.00519006 -0.31873044 -0.14907118  0.09185196]\n",
      " [ 0.13424099 -0.31209101  0.13113917  0.20629151 -0.71992843  0.11810456\n",
      "   0.18763768 -0.17140785  0.20518644  0.06272113]\n",
      " [ 0.0655409   0.19310855 -0.19386766  0.28861352 -0.09280006  0.19589783\n",
      "   0.34356717 -0.25716209 -0.07306113  0.07047695]\n",
      " [ 0.03155391  0.0173648  -0.3850587  -0.03490266 -0.02587279 -0.0057679\n",
      "  -0.25596613  0.07045889  0.5680273   0.22849784]\n",
      " [ 0.44250513  0.46558739  0.34171014  0.21249624 -0.2153908  -0.28669305\n",
      "   0.78319611 -0.05230185 -0.08721683  0.23291369]\n",
      " [-0.07531161  0.33987465  0.20570712  0.04369315 -0.14209198 -0.21149821\n",
      "   0.20410597  0.35005728  0.09773232  0.31115097]\n",
      " [-0.10555474  0.3796645   0.36905675  0.33764098 -0.36781468 -0.13480345\n",
      "   0.10852634 -0.08003747  0.05388399  0.30863312]\n",
      " [-0.11773552 -0.45807306 -0.32668429 -0.11973118 -0.47516974  0.02817382\n",
      "   0.15854544  0.00178497  0.27397683  0.4378792 ]\n",
      " [-0.01816295  0.45378234 -0.38289739  0.07363312 -0.43940097  0.27140173\n",
      "   0.19528971  0.30367654  0.19919392 -0.07598161]\n",
      " [ 0.0262119  -0.14436076 -0.13037404  0.26486987 -0.1038297   0.05623169\n",
      "  -0.08748376  0.1001065  -0.17908521  0.09656066]\n",
      " [ 0.0691039  -0.23241707 -0.53392449  0.15256858 -0.0046815   0.13463805\n",
      "   0.45371329 -0.16191763  0.05699954 -0.06818964]\n",
      " [ 0.09539954  0.0824747  -0.07398892 -0.20162251 -0.04584085 -0.13941276\n",
      "   0.03006174  0.32464782 -0.0938506  -0.25217245]\n",
      " [ 0.03981833 -0.36862946 -0.11445219  0.22464861 -0.17089463 -0.11159857\n",
      "   0.22839748 -0.04909604  0.17691511  0.00504407]\n",
      " [-0.0225978   0.01705397 -0.02658938  0.05677764  0.41496877 -0.15831275\n",
      "  -0.10169501 -0.08203909 -0.12692575  0.14393651]\n",
      " [-0.2007053   0.09934948  0.34100921  0.44679082 -0.05425789 -0.31762388\n",
      "   0.01792856  0.23595233 -0.02503573  0.1355694 ]\n",
      " [-0.17331375  0.15990001  0.12520139 -0.02795506  0.02563984 -0.06481585\n",
      "   0.42154188 -0.41561478 -0.2817213  -0.06060898]\n",
      " [ 0.0259138  -0.07580626 -0.04663813 -0.14636657 -0.29256347  0.15957169\n",
      "   0.08733793 -0.06570597 -0.14283201  0.14634785]\n",
      " [-0.24902997  0.21782723 -0.02264396 -0.0011271   0.11835272  0.25661646\n",
      "  -0.05922014  0.06256879  0.0209372   0.11164804]\n",
      " [-0.23180313 -0.33221696 -0.32123603  0.07749645 -0.37245039 -0.05924379\n",
      "  -0.07113166 -0.163594    0.39741597 -0.16111408]]\n",
      "[[-0.07838704 -0.27892254 -0.67110513 -0.18557917  0.32562265]\n",
      " [ 0.04015067  0.24940912  0.21511786 -0.28271379 -0.07839385]\n",
      " [-0.06221172 -0.46814861  0.0825393   0.35039945 -0.35976202]\n",
      " [-0.35988783 -0.00385476  0.23031026  0.08228085 -0.26413169]\n",
      " [-0.15744962  0.22581501  0.11223615  0.27788601 -0.24385723]\n",
      " [ 0.19313268 -0.3103007  -0.16355259 -0.28634115  0.56360649]\n",
      " [ 0.23768799  0.00533864 -0.51581982 -0.30504188 -0.23014754]\n",
      " [-0.43309628 -0.10869143  0.00671093 -0.12508294 -0.532577  ]\n",
      " [-0.03373843 -0.23431651 -0.10844036  0.14229218  0.4691556 ]\n",
      " [-0.03551028  0.31072669  0.26273659 -0.06542754  0.00435764]]\n",
      "(20, 10)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "std1=(1/np.sqrt(20))#standard deviation of initialiing first weights\n",
    "std2=(1/np.sqrt(h))#standard deviation of initialiing second weights\n",
    "w1=np.random.normal(0,std1,(20,h))#gaussian random assignment of weight\n",
    "w2=np.random.normal(0,std2,(h,5))#gaussian random assignment of weights \n",
    "#print(\"std1\\n\",std1,\"std2\\n\",std2)\n",
    "print(\"w1\\n\",w1)\n",
    "print(\"w2\\n\",w2)\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
